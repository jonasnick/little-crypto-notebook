\subsection{Prove Multiplication in ZK}

\begin{equation*}
  \mathcal{R}_{\textrm{mult}} = \{ ((G_0, G_1, H \in \mathbb{G}), (V, C \in \mathbb{G})) , (a, b, s_C, s_V \in \mathbb{Z}_p))  | V = abG_0 + s_VH, C = aG_0 + bG_1 + s_CH\}
\end{equation*}

\begin{enumerate}
\item Prover draws $s_a, s_b, s_S, \tau_1, \tau_2$ sends
  \begin{align*}
    S &= s_aG_0 + s_b G_1 + s_S H\\
    T_1 &= (as_b + bs_a)G_0 + \tau_1H\\
    T_2 &= s_as_bG_0 + \tau_2H\\
  \end{align*}
  \item Verifier sends random challenge $x$
  \item Prover sends
  \begin{align*}
    l = l(x) &= a + s_ax\\
    r = r(x) &= b + s_bx\\
    \mu &= s_C + xs_S\\
    \tau_x &= s_v + x\tau_1 + x^2\tau_2
  \end{align*}
  \item Verifier checks
    \begin{align*}
      C + xS &= lG_0 + rG_1 + \mu H\\
      V + xT_1 + x^2T_2 &= lrG_0 + \tau_x H \\
    \end{align*}
\end{enumerate}

\subsubsection{Correctness}
\begin{align*}
  C + xS &= aG_0 + bG_1 + s_CH + x(s_aG_0 + s_b G_1 + s_SH)\\
         &= (a + xs_a)G_0 + (b + xs_b)G_1 + (s_C + xs_S)H\\
         &= lG_0 + rG_1 + \mu H\\
\end{align*}
\begin{align*}
  V + xT_1 + x^2T_2 &= abG_0 + s_vH+ x(as_b + bs_a)G_0 + x\tau_1H + x^2s_as_bG_0 + x^2\tau_2H\\
                    &= (ab + x(as_b + bs_a) + x^2s_as_b)G_0 + (s_v + x\tau_1 + x^2\tau_2)H\\
                    &= lrG_0 + \tau_xH
\end{align*}

\subsubsection{Perfect Honest Verifier Zero Knowledge}

\paragraph{Proof sketch}
\begin{enumerate}
  \item Simulator draws random $l, r, \mu, \tau_x, T_2$. And since we're proving honest verifier ZK, the simulator draws random $x$.
  \item Simulator sets $S = \frac{1}{x}(lG_0 + rG_1 + \mu H - C)$
  \item Simulator sets $T_1 = \frac{1}{x}(lrG_0 + \tau_xH - V - x^2T_2$)
\end{enumerate}
All elements in the proof are either independently randomly distributed or their relationship is fully defined by the verification equations.
The simulator is efficient.

\paragraph{Variants of the Protocol that are not (Perfect) HVZK}
Note that in our definition of HVZK, the adversary comes up with the statement and the witness.
A ZK proof like the one above seems tricky because these variants don't obviously look (to me) like they violate the proof.

\begin{itemize}
  \item Can't we just remove $s_a$ from the protocol, i.e., the prover sends $l = l(x) = a$? No, even if the simulator is able to make the verification equations work for some $l$ drawn at random, the adversary knows the actual witness and easily notices that $l \ne a$.
  \item Can't we just remove $s_S$ from the protocol, i.e., the prover sends $\mu = s_C$? No, the same reason why we can't remove $s_a$ applies as $s_C$ is also a witness..
  \item Can't we remove $\tau_2$ from the protocol, i.e., the prover sets $T_2 = s_as_bG_0$? No, that would allow the adversary to compute $s_m$ from $T_2 = s_mG_0$. The adversary can also compute $s_a$ from $l$ and $s_b$ from $r$ and it will see that $s_as_b \ne s_m$.
\end{itemize}

\begin{theorem}
  Above protocol for $\mathcal{R}_{\textrm{mult}}$ has perfect HVZK.
\end{theorem}
\begin{pproof}
  \step{1}{It suffices to give a simulator that outputs a transcript $\pi = (x, l, r, \mu, \tau_x, S, T_1, T_2)$ such that the probability distribution $\Pr(\pi' = \pi)$ for random variable $\pi'$ is equal to the probability distribution of an accepting transcript for a correct statement between a prover and honest verifier.
           To simplify the notation, we write $\Pr(\pi' = \pi)$ as $\Pr(\pi)$.}
  \step{2}{ Let $\mathcal{S}$ be a simulator which draws $x, l, r, \mu, \tau_x, T_2$ uniformly at random and sets
    \begin{align*}
      S &:= \frac{1}{x}(lG_0 + rG_1 + \mu H - C)\\
      T_1 &:= \frac{1}{x}(lrG_0 + \tau_xH - V - x^2T_2)
    \end{align*}
    The simulator is efficient.}
  \step{3}{ The distribution of transcript $\pi$ output by the simulator is
    \begin{align*}
        \Pr(\pi) &= \Pr(x, l, r, \mu, \tau_x, S, T_1, T_2)\\
                        &= \begin{cases} 0 \text{ if } S \ne \frac{1}{x}(lG_0 + rG_1 + \mu H - C) \text{ or } T_1 \ne \frac{1}{x}(lrG_0 + \tau_xH - V - x^2T_2) \\
                             \Pr(x)\Pr(l)\Pr(r)\Pr(\mu)\Pr(\tau_x)\Pr(T_2) \text{ otherwise}
                             \end{cases}
    \end{align*}
    where $\Pr(x),\Pr(l),\Pr(r),\Pr(\mu),\Pr(\tau_x),\Pr(T_2)$ are uniform distributions.
    }
  \step{4}{The distribution of transcript $\pi$ in an honest interaction is
    \begin{align*}
      \Pr(\pi) &= \Pr(x) \Pr(l|x, s_a) \Pr(r|x, s_b) \Pr(\mu| x, s_S) \Pr(\tau_x| x, \tau_1, \tau_2) \Pr(S| s_a, s_b, s_S) \Pr(T_1| s_a, s_b, \tau_1)\\
      &\qquad \Pr(T_2| s_a, s_b, \tau_2) \Pr(s_a)\Pr(s_b)\Pr(s_S)\Pr(\tau_1 )\Pr(\tau_2)
    \end{align*}
    where $\Pr(x),\Pr(s_a),\Pr(s_b),\Pr(s_S),\Pr(\tau_1),\Pr(\tau_2)$ are uniform distributions and all conditional probability distributions are 0 if the defining equation holds and 1 otherwise. For example, $\Pr(l|x, s_a) = 0$ if $l\ne a + s_ax$.}
   \step{5}{We can rewrite the distribution of transcript $\pi$ in an honest interaction as
    \begin{align*}
        \Pr(\pi) &= \Pr(x, l, r, \mu, \tau_x, S, T_1, T_2)\\
                        &= \begin{cases} 0 \text{ if } S \ne s_aG_0 + s_bG_1 x^{-1}(s_C - \mu)H \text{ or } T_1 \ne (as_b + bs_a)G_0 + \tau_1H \text{ for }\\ \qquad s_a := x^{-1}(l - a), s_b := x^{-1}(r - b), \tau_1 := - x^{-1}(s_V  + x^2\tau_2 - \tau_x), \tau_2 := \log_H(T_2) - s_as_b\log_H(G_0) \\
                             \Pr(x)\Pr(l)\Pr(r)\Pr(\mu)\Pr(\tau_x)\Pr(T_2) \text{ otherwise}
                             \end{cases}
    \end{align*}
    where $\Pr(x),\Pr(l),\Pr(r),\Pr(\mu),\Pr(\tau_x),\Pr(T_2)$ are uniform distributions.
   }
   \begin{pproof}
     \step{5a}{Since $\Pr(l|x, s_a)\Pr(s_a) = \Pr(l,s_a|x) = \Pr(s_a|x, l) \Pr(l)$ and that holds similarly for $r$, $\mu$, as well as $\Pr(\tau_x| x, \tau_1, \tau_2)\Pr(\tau_1 ) = \Pr(\tau_1| x, \tau_x, \tau_2)\Pr(\tau_x )$ and $\Pr(T_2| s_a, s_b, \tau_2)\Pr(\tau_2) = \Pr(\tau_2| s_a, s_b, T_2)\Pr(T_2)$,
       we have
       \begin{align*}
      \Pr(\pi) &= \Pr(x) \Pr(s_a|x, l) \Pr(s_b|x, r) \Pr(s_S| x, \mu) \Pr(\tau_1| x, \tau_x, \tau_2) \Pr(S| s_a, s_b, s_S) \Pr(T_1| s_a, s_b, \tau_1)\\
      &\qquad \Pr(\tau_2| s_a, s_b, T_2) \Pr(l)\Pr(r)\Pr(\mu)\Pr(\tau_x )\Pr(T_2)
       \end{align*}
       }
     \step{5b}{We can rewrite this to get rid of hidden random variables $s_a, s_b, s_S, \tau_1, \tau_2$ as
       \begin{align*}
      \Pr(\pi) &= \Pr(x) \Pr(S| x, l, r, \mu) \Pr(T_1| x, l, r, \tau_x, T_2) \Pr(l)\Pr(r)\Pr(\mu)\Pr(\tau_x )\Pr(T_2).
       \end{align*}
     }
     \step{5c}{$\Pr(\pi) = 0$ if and only if $\Pr(S| x, l, r, \mu) = 0$ or $\Pr(T_1| x, l, r, \tau_x, T_2) = 0$, otherwise both probabilities are 1.}
    \end{pproof}
  \step{6}{The distributions of the honest transcript and simulated transcripts are the same.}
  \begin{pproof}
    \step{6a}{In the honest transcript we have $S = \frac{1}{x}(lG_0 + rG_1 + s_CH - (aG_0 + bG_1 + \mu H)) = \frac{1}{x}(lG_0 + rG_1 + s_CH - C)$ since the statement is valid. Similarly
      \begin{align*}
        T_1 &= (as_b + bs_a)G_0 - x^{-1}(s_V  + x^2(\log_H(T_2) - s_as_b\log_H(G_0)) - \tau_x)H\\
            &= \frac{1}{x}((xas_b + xbs_a)G_0 + \tau_xH - s_VH - x^2T_2 - x^2s_as_bG_0)\\
            &= \frac{1}{x}((xas_b + xbs_a - x^2s_as_b)G_0 + \tau_xH - s_VH - x^2T_2)\\
            &= \frac{1}{x}((lr - ab)G_0 + \tau_xH - s_VH - x^2T_2)\\
            &= \frac{1}{x}(lr G_0 + \tau_xH - V - x^2T_2)\\
      \end{align*}}
  \end{pproof}
\end{pproof}